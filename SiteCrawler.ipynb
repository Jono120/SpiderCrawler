{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7258b5f2",
   "metadata": {},
   "source": [
    "# Site Crawler\n",
    "A Python script to analyze a website's links and sitemap, now with link status checking.\n",
    "- To run this script, you may need to install the required libraries.\n",
    "    - If there is packages needed just uncomment the ones you need and run with the script build\n",
    "\n",
    "The below notebook was built with Python 3.12 as a base runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd940ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lxml in c:\\users\\lakejo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (6.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following command(s) to install any required libraries:\n",
    "%pip install --upgrade pip\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b90c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Set, List, Dict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb61011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set to store visited URLs to prevent infinite loops on circular links\n",
    "visited_urls: Set[str] = set()\n",
    "\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a URL is valid by parsing it and ensuring it has a scheme and netloc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ce826",
   "metadata": {},
   "source": [
    "## Link Finder\n",
    "This will scan the base domain and check for the level of internal links within the site.\n",
    "It runs a recursive check on the webpage as well as its subpages (children) to provide clear detail to the domain structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f64592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_links(url: str, base_domain: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Recursively finds all internal links on a given webpage and its subpages.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the page to crawl.\n",
    "        base_domain (str): The domain of the website to stay within.\n",
    "    \n",
    "    Returns:\n",
    "        Set[str]: A set of unique internal URLs found.\n",
    "    \"\"\"\n",
    "    if url in visited_urls:\n",
    "        return set()\n",
    "    \n",
    "    print(f\"Crawling: {url}\")\n",
    "    # Add a limit to the number of URLs to crawl to prevent the script from running forever\n",
    "    # adjust the limit for the number of URLs to check as needed\n",
    "    if len(visited_urls) > 10000000:\n",
    "        print(\"Crawl limit reached (1000 URLs). Stopping further crawling.\")\n",
    "        return set()\n",
    "    \n",
    "    visited_urls.add(url)\n",
    "    \n",
    "    internal_links: Set[str] = set()\n",
    "    try:\n",
    "        response = requests.get(url, timeout=500)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            full_url = urljoin(url, href)\n",
    "            \n",
    "            # Normalize URL to remove fragments and query parameters\n",
    "            normalized_url = urlparse(full_url)._replace(fragment='', query='').geturl()\n",
    "            \n",
    "            if is_valid_url(normalized_url) and urlparse(normalized_url).netloc == base_domain:\n",
    "                internal_links.add(normalized_url)\n",
    "                \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "        \n",
    "    return internal_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc353b93",
   "metadata": {},
   "source": [
    "## Sitemap check\n",
    "The below section will scan the domain for a sitemap file that can be used as a starting point to what is contained within the site.\n",
    "It will produce the number of what is contained in the sitemap and provide it to the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ce088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sitemap(url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Attempts to find and parse a sitemap.xml file for a given URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The base URL of the website.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of URLs found within the sitemap, or an empty list if not found.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    sitemap_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/sitemap.xml\"\n",
    "    \n",
    "    print(f\"\\nAttempting to find sitemap at: {sitemap_url}\")\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=500)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        urls_from_sitemap = [loc.text for loc in soup.find_all('loc')]\n",
    "        \n",
    "        if urls_from_sitemap:\n",
    "            print(f\"Found {len(urls_from_sitemap)} URLs in the sitemap.\")\n",
    "            return urls_from_sitemap\n",
    "        else:\n",
    "            print(\"Sitemap found but no URLs were listed.\")\n",
    "            return []\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Sitemap not found or an error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d136d7",
   "metadata": {},
   "source": [
    "## Progress Bar\n",
    "Just a simple progress bar that will show how many pages scanned and how much to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b219b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link_status(urls: Set[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Checks the HTTP status for a set of URLs.\n",
    "    \n",
    "    Args:\n",
    "        urls (Set[str]): A set of URLs to check.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary of URLs and their status codes or error messages.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"CHECKING LINK STATUSES...\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    status_results = {}\n",
    "    total_links = len(urls)\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        # Progress indicator\n",
    "        sys.stdout.write(f\"\\rChecking link {i}/{total_links}: {url[:60000]}...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        try:\n",
    "            response = requests.head(url, timeout=500, allow_redirects=True)\n",
    "            status_results[url] = str(response.status_code)\n",
    "        except requests.RequestException as e:\n",
    "            status_results[url] = f\"Error: {e}\"\n",
    "            \n",
    "    sys.stdout.write(\"\\n\") # Newline after progress bar\n",
    "    return status_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35841a18",
   "metadata": {},
   "source": [
    "## URL Request and Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbbd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for website: https://www.wgtn.ac.nz\n",
      "------------------------------\n",
      "Crawling: https://www.wgtn.ac.nz\n",
      "\n",
      "Attempting to find sitemap at: https://www.wgtn.ac.nz/sitemap.xml\n",
      "Found 3 URLs in the sitemap.\n",
      "\n",
      "==============================\n",
      "ANALYSIS RESULTS\n",
      "==============================\n",
      "\n",
      "Links found by crawling https://www.wgtn.ac.nz and its internal pages:\n",
      "- https://www.wgtn.ac.nz\n",
      "- https://www.wgtn.ac.nz/\n",
      "- https://www.wgtn.ac.nz/LinkedIn\n",
      "- https://www.wgtn.ac.nz/YouTube\n",
      "- https://www.wgtn.ac.nz/about\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses/auckland\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses/kelburn\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses/pipitea\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses/te-aro\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/faculties-schools\n",
      "- https://www.wgtn.ac.nz/about/contacts\n",
      "- https://www.wgtn.ac.nz/about/contacts/media-enquiries\n",
      "- https://www.wgtn.ac.nz/about/governance/university-publications\n",
      "- https://www.wgtn.ac.nz/about/our-story/news-events/social-media\n",
      "- https://www.wgtn.ac.nz/about/working-here\n",
      "- https://www.wgtn.ac.nz/accommodation\n",
      "- https://www.wgtn.ac.nz/accommodation/halls\n",
      "- https://www.wgtn.ac.nz/business\n",
      "- https://www.wgtn.ac.nz/engage\n",
      "- https://www.wgtn.ac.nz/engage/alumni\n",
      "- https://www.wgtn.ac.nz/engage/alumni/stay-connected\n",
      "- https://www.wgtn.ac.nz/engage/giving\n",
      "- https://www.wgtn.ac.nz/engage/giving/donate\n",
      "- https://www.wgtn.ac.nz/engage/giving/priorities/student-support/alumni-appeal\n",
      "- https://www.wgtn.ac.nz/engage/global\n",
      "- https://www.wgtn.ac.nz/engage/industry-government\n",
      "- https://www.wgtn.ac.nz/engage/public\n",
      "- https://www.wgtn.ac.nz/events\n",
      "- https://www.wgtn.ac.nz/events/2026/01/postgraduate-information-webinar\n",
      "- https://www.wgtn.ac.nz/facebook\n",
      "- https://www.wgtn.ac.nz/fehps\n",
      "- https://www.wgtn.ac.nz/fgr\n",
      "- https://www.wgtn.ac.nz/fhss\n",
      "- https://www.wgtn.ac.nz/healthandsafety\n",
      "- https://www.wgtn.ac.nz/healthandsafety/emergency\n",
      "- https://www.wgtn.ac.nz/home\n",
      "- https://www.wgtn.ac.nz/home/_edit\n",
      "- https://www.wgtn.ac.nz/instagram\n",
      "- https://www.wgtn.ac.nz/international\n",
      "- https://www.wgtn.ac.nz/international/applying\n",
      "- https://www.wgtn.ac.nz/international/current-international-students\n",
      "- https://www.wgtn.ac.nz/international/prepare-to-study\n",
      "- https://www.wgtn.ac.nz/international/scholarships-fees\n",
      "- https://www.wgtn.ac.nz/international/study-options\n",
      "- https://www.wgtn.ac.nz/international/why-wellington\n",
      "- https://www.wgtn.ac.nz/jobs\n",
      "- https://www.wgtn.ac.nz/law\n",
      "- https://www.wgtn.ac.nz/library\n",
      "- https://www.wgtn.ac.nz/maori-at-victoria\n",
      "- https://www.wgtn.ac.nz/menu-nav-test/home\n",
      "- https://www.wgtn.ac.nz/news\n",
      "- https://www.wgtn.ac.nz/news/2025/12/first-doctorate-of-nursing\n",
      "- https://www.wgtn.ac.nz/news/2025/12/ni-vanuatu-manaaki-scholar-brings-biomedical-expertise-home-with-her\n",
      "- https://www.wgtn.ac.nz/pasifika\n",
      "- https://www.wgtn.ac.nz/puaha\n",
      "- https://www.wgtn.ac.nz/research\n",
      "- https://www.wgtn.ac.nz/research/partnerships\n",
      "- https://www.wgtn.ac.nz/research/research-degrees\n",
      "- https://www.wgtn.ac.nz/research/researchers\n",
      "- https://www.wgtn.ac.nz/research/researchers/experts\n",
      "- https://www.wgtn.ac.nz/research/strengths\n",
      "- https://www.wgtn.ac.nz/research/strengths/performance-and-rankings\n",
      "- https://www.wgtn.ac.nz/research/support\n",
      "- https://www.wgtn.ac.nz/scholarships\n",
      "- https://www.wgtn.ac.nz/scholarships/scholarships\n",
      "- https://www.wgtn.ac.nz/schools\n",
      "- https://www.wgtn.ac.nz/science-engineering\n",
      "- https://www.wgtn.ac.nz/service\n",
      "- https://www.wgtn.ac.nz/site-info\n",
      "- https://www.wgtn.ac.nz/site-info/feedback\n",
      "- https://www.wgtn.ac.nz/site-info/glossary\n",
      "- https://www.wgtn.ac.nz/site-info/privacy\n",
      "- https://www.wgtn.ac.nz/site-info/site-map\n",
      "- https://www.wgtn.ac.nz/sitemap-courses.xml\n",
      "- https://www.wgtn.ac.nz/staff\n",
      "- https://www.wgtn.ac.nz/students\n",
      "- https://www.wgtn.ac.nz/students/graduation-and-careers\n",
      "- https://www.wgtn.ac.nz/students/money\n",
      "- https://www.wgtn.ac.nz/students/money/financial-survival\n",
      "- https://www.wgtn.ac.nz/students/new-students\n",
      "- https://www.wgtn.ac.nz/students/student-life\n",
      "- https://www.wgtn.ac.nz/students/study\n",
      "- https://www.wgtn.ac.nz/students/study/dates\n",
      "- https://www.wgtn.ac.nz/students/study/enrolment/courses\n",
      "- https://www.wgtn.ac.nz/students/support\n",
      "- https://www.wgtn.ac.nz/students/tools-and-help\n",
      "- https://www.wgtn.ac.nz/study\n",
      "- https://www.wgtn.ac.nz/study/apply-enrol\n",
      "- https://www.wgtn.ac.nz/study/course-planning\n",
      "- https://www.wgtn.ac.nz/study/events-visits\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses/postgraduate\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses/subjects\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses/undergraduate\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses/undergraduate/taster-lectures\n",
      "- https://www.wgtn.ac.nz/study/student-finance\n",
      "- https://www.wgtn.ac.nz/study/university-life\n",
      "- https://www.wgtn.ac.nz/teaching-support\n",
      "- https://www.wgtn.ac.nz/teaching-support/teaching-with-technology/log-in-to-nuku\n",
      "- https://www.wgtn.ac.nz/tiktok\n",
      "- https://www.wgtn.ac.nz/wfadi\n",
      "------------------------------\n",
      "\n",
      "URLs found in the sitemap:\n",
      "- https://www.wgtn.ac.nz/topics-sitemap.xml\n",
      "- https://www.wgtn.ac.nz/sitemap-people.xml\n",
      "- https://www.wgtn.ac.nz/sitemap-courses.xml\n",
      "------------------------------\n",
      "\n",
      "Checking the status of all unique URLs found...\n",
      "\n",
      "==============================\n",
      "CHECKING LINK STATUSES...\n",
      "==============================\n",
      "Checking link 104/104: https://www.wgtn.ac.nz/students/student-life...ns......d-rankings...lectures.....xpertise-home-with-her...\n",
      "\n",
      "==============================\n",
      "BROKEN LINK REPORT\n",
      "==============================\n",
      "[999] https://www.wgtn.ac.nz/LinkedIn\n",
      "[403] https://www.wgtn.ac.nz/engage/giving/donate\n",
      "[404] https://www.wgtn.ac.nz/menu-nav-test/home\n",
      "[404] https://www.wgtn.ac.nz/puaha\n",
      "[403] https://www.wgtn.ac.nz/site-info\n",
      "[403] https://www.wgtn.ac.nz/staff\n",
      "\n",
      "==============================\n",
      "CRAWLED URLS PARENT/SUBLINK COUNTS\n",
      "==============================\n",
      "Total parents: 31\n",
      "Total URLs: 102\n",
      "- https://www.wgtn.ac.nz/: 37 sublinks\n",
      "    - https://www.wgtn.ac.nz\n",
      "    - https://www.wgtn.ac.nz/\n",
      "    - https://www.wgtn.ac.nz/LinkedIn\n",
      "    - https://www.wgtn.ac.nz/YouTube\n",
      "    - https://www.wgtn.ac.nz/about\n",
      "    - https://www.wgtn.ac.nz/accommodation\n",
      "    - https://www.wgtn.ac.nz/business\n",
      "    - https://www.wgtn.ac.nz/engage\n",
      "    - https://www.wgtn.ac.nz/events\n",
      "    - https://www.wgtn.ac.nz/facebook\n",
      "    - https://www.wgtn.ac.nz/fehps\n",
      "    - https://www.wgtn.ac.nz/fgr\n",
      "    - https://www.wgtn.ac.nz/fhss\n",
      "    - https://www.wgtn.ac.nz/healthandsafety\n",
      "    - https://www.wgtn.ac.nz/home\n",
      "    - https://www.wgtn.ac.nz/instagram\n",
      "    - https://www.wgtn.ac.nz/international\n",
      "    - https://www.wgtn.ac.nz/jobs\n",
      "    - https://www.wgtn.ac.nz/law\n",
      "    - https://www.wgtn.ac.nz/library\n",
      "    - https://www.wgtn.ac.nz/maori-at-victoria\n",
      "    - https://www.wgtn.ac.nz/news\n",
      "    - https://www.wgtn.ac.nz/pasifika\n",
      "    - https://www.wgtn.ac.nz/puaha\n",
      "    - https://www.wgtn.ac.nz/research\n",
      "    - https://www.wgtn.ac.nz/scholarships\n",
      "    - https://www.wgtn.ac.nz/schools\n",
      "    - https://www.wgtn.ac.nz/science-engineering\n",
      "    - https://www.wgtn.ac.nz/service\n",
      "    - https://www.wgtn.ac.nz/site-info\n",
      "    - https://www.wgtn.ac.nz/sitemap-courses.xml\n",
      "    - https://www.wgtn.ac.nz/staff\n",
      "    - https://www.wgtn.ac.nz/students\n",
      "    - https://www.wgtn.ac.nz/study\n",
      "    - https://www.wgtn.ac.nz/teaching-support\n",
      "    - https://www.wgtn.ac.nz/tiktok\n",
      "    - https://www.wgtn.ac.nz/wfadi\n",
      "- https://www.wgtn.ac.nz/about: 2 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/contacts\n",
      "    - https://www.wgtn.ac.nz/about/working-here\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities: 2 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/faculties-schools\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses: 4 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/auckland\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/kelburn\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/pipitea\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/te-aro\n",
      "- https://www.wgtn.ac.nz/about/contacts: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/contacts/media-enquiries\n",
      "- https://www.wgtn.ac.nz/about/governance: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/governance/university-publications\n",
      "- https://www.wgtn.ac.nz/about/our-story/news-events: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/our-story/news-events/social-media\n",
      "- https://www.wgtn.ac.nz/accommodation: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/accommodation/halls\n",
      "- https://www.wgtn.ac.nz/engage: 5 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/alumni\n",
      "    - https://www.wgtn.ac.nz/engage/giving\n",
      "    - https://www.wgtn.ac.nz/engage/global\n",
      "    - https://www.wgtn.ac.nz/engage/industry-government\n",
      "    - https://www.wgtn.ac.nz/engage/public\n",
      "- https://www.wgtn.ac.nz/engage/alumni: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/alumni/stay-connected\n",
      "- https://www.wgtn.ac.nz/engage/giving: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/giving/donate\n",
      "- https://www.wgtn.ac.nz/engage/giving/priorities/student-support: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/giving/priorities/student-support/alumni-appeal\n",
      "- https://www.wgtn.ac.nz/events/2026/01: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/events/2026/01/postgraduate-information-webinar\n",
      "- https://www.wgtn.ac.nz/healthandsafety: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/healthandsafety/emergency\n",
      "- https://www.wgtn.ac.nz/home: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/home/_edit\n",
      "- https://www.wgtn.ac.nz/international: 6 sublinks\n",
      "    - https://www.wgtn.ac.nz/international/applying\n",
      "    - https://www.wgtn.ac.nz/international/current-international-students\n",
      "    - https://www.wgtn.ac.nz/international/prepare-to-study\n",
      "    - https://www.wgtn.ac.nz/international/scholarships-fees\n",
      "    - https://www.wgtn.ac.nz/international/study-options\n",
      "    - https://www.wgtn.ac.nz/international/why-wellington\n",
      "- https://www.wgtn.ac.nz/menu-nav-test: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/menu-nav-test/home\n",
      "- https://www.wgtn.ac.nz/news/2025/12: 2 sublinks\n",
      "    - https://www.wgtn.ac.nz/news/2025/12/first-doctorate-of-nursing\n",
      "    - https://www.wgtn.ac.nz/news/2025/12/ni-vanuatu-manaaki-scholar-brings-biomedical-expertise-home-with-her\n",
      "- https://www.wgtn.ac.nz/research: 5 sublinks\n",
      "    - https://www.wgtn.ac.nz/research/partnerships\n",
      "    - https://www.wgtn.ac.nz/research/research-degrees\n",
      "    - https://www.wgtn.ac.nz/research/researchers\n",
      "    - https://www.wgtn.ac.nz/research/strengths\n",
      "    - https://www.wgtn.ac.nz/research/support\n",
      "- https://www.wgtn.ac.nz/research/researchers: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/research/researchers/experts\n",
      "- https://www.wgtn.ac.nz/research/strengths: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/research/strengths/performance-and-rankings\n",
      "- https://www.wgtn.ac.nz/scholarships: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/scholarships/scholarships\n",
      "- https://www.wgtn.ac.nz/site-info: 4 sublinks\n",
      "    - https://www.wgtn.ac.nz/site-info/feedback\n",
      "    - https://www.wgtn.ac.nz/site-info/glossary\n",
      "    - https://www.wgtn.ac.nz/site-info/privacy\n",
      "    - https://www.wgtn.ac.nz/site-info/site-map\n",
      "- https://www.wgtn.ac.nz/students: 7 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/graduation-and-careers\n",
      "    - https://www.wgtn.ac.nz/students/money\n",
      "    - https://www.wgtn.ac.nz/students/new-students\n",
      "    - https://www.wgtn.ac.nz/students/student-life\n",
      "    - https://www.wgtn.ac.nz/students/study\n",
      "    - https://www.wgtn.ac.nz/students/support\n",
      "    - https://www.wgtn.ac.nz/students/tools-and-help\n",
      "- https://www.wgtn.ac.nz/students/money: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/money/financial-survival\n",
      "- https://www.wgtn.ac.nz/students/study: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/study/dates\n",
      "- https://www.wgtn.ac.nz/students/study/enrolment: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/study/enrolment/courses\n",
      "- https://www.wgtn.ac.nz/study: 6 sublinks\n",
      "    - https://www.wgtn.ac.nz/study/apply-enrol\n",
      "    - https://www.wgtn.ac.nz/study/course-planning\n",
      "    - https://www.wgtn.ac.nz/study/events-visits\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses\n",
      "    - https://www.wgtn.ac.nz/study/student-finance\n",
      "    - https://www.wgtn.ac.nz/study/university-life\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses: 3 sublinks\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/postgraduate\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/subjects\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/undergraduate\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses/undergraduate: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/undergraduate/taster-lectures\n",
      "- https://www.wgtn.ac.nz/teaching-support/teaching-with-technology: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/teaching-support/teaching-with-technology/log-in-to-nuku\n",
      "\n",
      "==============================\n",
      "SITEMAP URLS PARENT/SUBLINK COUNTS\n",
      "==============================\n",
      "Total parents: 1\n",
      "Total URLs: 3\n",
      "- https://www.wgtn.ac.nz/: 3 sublinks\n",
      "    - https://www.wgtn.ac.nz/sitemap-courses.xml\n",
      "    - https://www.wgtn.ac.nz/sitemap-people.xml\n",
      "    - https://www.wgtn.ac.nz/topics-sitemap.xml\n",
      "\n",
      "==============================\n",
      "ALL URLS PARENT/SUBLINK COUNTS\n",
      "==============================\n",
      "Total parents: 31\n",
      "Total URLs: 104\n",
      "- https://www.wgtn.ac.nz/: 39 sublinks\n",
      "    - https://www.wgtn.ac.nz\n",
      "    - https://www.wgtn.ac.nz/\n",
      "    - https://www.wgtn.ac.nz/LinkedIn\n",
      "    - https://www.wgtn.ac.nz/YouTube\n",
      "    - https://www.wgtn.ac.nz/about\n",
      "    - https://www.wgtn.ac.nz/accommodation\n",
      "    - https://www.wgtn.ac.nz/business\n",
      "    - https://www.wgtn.ac.nz/engage\n",
      "    - https://www.wgtn.ac.nz/events\n",
      "    - https://www.wgtn.ac.nz/facebook\n",
      "    - https://www.wgtn.ac.nz/fehps\n",
      "    - https://www.wgtn.ac.nz/fgr\n",
      "    - https://www.wgtn.ac.nz/fhss\n",
      "    - https://www.wgtn.ac.nz/healthandsafety\n",
      "    - https://www.wgtn.ac.nz/home\n",
      "    - https://www.wgtn.ac.nz/instagram\n",
      "    - https://www.wgtn.ac.nz/international\n",
      "    - https://www.wgtn.ac.nz/jobs\n",
      "    - https://www.wgtn.ac.nz/law\n",
      "    - https://www.wgtn.ac.nz/library\n",
      "    - https://www.wgtn.ac.nz/maori-at-victoria\n",
      "    - https://www.wgtn.ac.nz/news\n",
      "    - https://www.wgtn.ac.nz/pasifika\n",
      "    - https://www.wgtn.ac.nz/puaha\n",
      "    - https://www.wgtn.ac.nz/research\n",
      "    - https://www.wgtn.ac.nz/scholarships\n",
      "    - https://www.wgtn.ac.nz/schools\n",
      "    - https://www.wgtn.ac.nz/science-engineering\n",
      "    - https://www.wgtn.ac.nz/service\n",
      "    - https://www.wgtn.ac.nz/site-info\n",
      "    - https://www.wgtn.ac.nz/sitemap-courses.xml\n",
      "    - https://www.wgtn.ac.nz/sitemap-people.xml\n",
      "    - https://www.wgtn.ac.nz/staff\n",
      "    - https://www.wgtn.ac.nz/students\n",
      "    - https://www.wgtn.ac.nz/study\n",
      "    - https://www.wgtn.ac.nz/teaching-support\n",
      "    - https://www.wgtn.ac.nz/tiktok\n",
      "    - https://www.wgtn.ac.nz/topics-sitemap.xml\n",
      "    - https://www.wgtn.ac.nz/wfadi\n",
      "- https://www.wgtn.ac.nz/about: 2 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/contacts\n",
      "    - https://www.wgtn.ac.nz/about/working-here\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities: 2 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/faculties-schools\n",
      "- https://www.wgtn.ac.nz/about/campuses-facilities/campuses: 4 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/auckland\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/kelburn\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/pipitea\n",
      "    - https://www.wgtn.ac.nz/about/campuses-facilities/campuses/te-aro\n",
      "- https://www.wgtn.ac.nz/about/contacts: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/contacts/media-enquiries\n",
      "- https://www.wgtn.ac.nz/about/governance: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/governance/university-publications\n",
      "- https://www.wgtn.ac.nz/about/our-story/news-events: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/about/our-story/news-events/social-media\n",
      "- https://www.wgtn.ac.nz/accommodation: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/accommodation/halls\n",
      "- https://www.wgtn.ac.nz/engage: 5 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/alumni\n",
      "    - https://www.wgtn.ac.nz/engage/giving\n",
      "    - https://www.wgtn.ac.nz/engage/global\n",
      "    - https://www.wgtn.ac.nz/engage/industry-government\n",
      "    - https://www.wgtn.ac.nz/engage/public\n",
      "- https://www.wgtn.ac.nz/engage/alumni: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/alumni/stay-connected\n",
      "- https://www.wgtn.ac.nz/engage/giving: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/giving/donate\n",
      "- https://www.wgtn.ac.nz/engage/giving/priorities/student-support: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/engage/giving/priorities/student-support/alumni-appeal\n",
      "- https://www.wgtn.ac.nz/events/2026/01: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/events/2026/01/postgraduate-information-webinar\n",
      "- https://www.wgtn.ac.nz/healthandsafety: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/healthandsafety/emergency\n",
      "- https://www.wgtn.ac.nz/home: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/home/_edit\n",
      "- https://www.wgtn.ac.nz/international: 6 sublinks\n",
      "    - https://www.wgtn.ac.nz/international/applying\n",
      "    - https://www.wgtn.ac.nz/international/current-international-students\n",
      "    - https://www.wgtn.ac.nz/international/prepare-to-study\n",
      "    - https://www.wgtn.ac.nz/international/scholarships-fees\n",
      "    - https://www.wgtn.ac.nz/international/study-options\n",
      "    - https://www.wgtn.ac.nz/international/why-wellington\n",
      "- https://www.wgtn.ac.nz/menu-nav-test: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/menu-nav-test/home\n",
      "- https://www.wgtn.ac.nz/news/2025/12: 2 sublinks\n",
      "    - https://www.wgtn.ac.nz/news/2025/12/first-doctorate-of-nursing\n",
      "    - https://www.wgtn.ac.nz/news/2025/12/ni-vanuatu-manaaki-scholar-brings-biomedical-expertise-home-with-her\n",
      "- https://www.wgtn.ac.nz/research: 5 sublinks\n",
      "    - https://www.wgtn.ac.nz/research/partnerships\n",
      "    - https://www.wgtn.ac.nz/research/research-degrees\n",
      "    - https://www.wgtn.ac.nz/research/researchers\n",
      "    - https://www.wgtn.ac.nz/research/strengths\n",
      "    - https://www.wgtn.ac.nz/research/support\n",
      "- https://www.wgtn.ac.nz/research/researchers: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/research/researchers/experts\n",
      "- https://www.wgtn.ac.nz/research/strengths: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/research/strengths/performance-and-rankings\n",
      "- https://www.wgtn.ac.nz/scholarships: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/scholarships/scholarships\n",
      "- https://www.wgtn.ac.nz/site-info: 4 sublinks\n",
      "    - https://www.wgtn.ac.nz/site-info/feedback\n",
      "    - https://www.wgtn.ac.nz/site-info/glossary\n",
      "    - https://www.wgtn.ac.nz/site-info/privacy\n",
      "    - https://www.wgtn.ac.nz/site-info/site-map\n",
      "- https://www.wgtn.ac.nz/students: 7 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/graduation-and-careers\n",
      "    - https://www.wgtn.ac.nz/students/money\n",
      "    - https://www.wgtn.ac.nz/students/new-students\n",
      "    - https://www.wgtn.ac.nz/students/student-life\n",
      "    - https://www.wgtn.ac.nz/students/study\n",
      "    - https://www.wgtn.ac.nz/students/support\n",
      "    - https://www.wgtn.ac.nz/students/tools-and-help\n",
      "- https://www.wgtn.ac.nz/students/money: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/money/financial-survival\n",
      "- https://www.wgtn.ac.nz/students/study: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/study/dates\n",
      "- https://www.wgtn.ac.nz/students/study/enrolment: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/students/study/enrolment/courses\n",
      "- https://www.wgtn.ac.nz/study: 6 sublinks\n",
      "    - https://www.wgtn.ac.nz/study/apply-enrol\n",
      "    - https://www.wgtn.ac.nz/study/course-planning\n",
      "    - https://www.wgtn.ac.nz/study/events-visits\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses\n",
      "    - https://www.wgtn.ac.nz/study/student-finance\n",
      "    - https://www.wgtn.ac.nz/study/university-life\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses: 3 sublinks\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/postgraduate\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/subjects\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/undergraduate\n",
      "- https://www.wgtn.ac.nz/study/programmes-courses/undergraduate: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/study/programmes-courses/undergraduate/taster-lectures\n",
      "- https://www.wgtn.ac.nz/teaching-support/teaching-with-technology: 1 sublinks\n",
      "    - https://www.wgtn.ac.nz/teaching-support/teaching-with-technology/log-in-to-nuku\n"
     ]
    }
   ],
   "source": [
    "# Requests the user to input a that is wanted to be checked\n",
    "if __name__ == \"__main__\":\n",
    "    # This will ask the user for the URL to analyse\n",
    "    start_url = input(\"Please enter the website URL to check (e.g., https://example.com): \")\n",
    "    if not start_url.startswith('http'):\n",
    "        start_url = 'https://' + start_url\n",
    "    \n",
    "    base_domain = urlparse(start_url).netloc\n",
    "    if not base_domain:\n",
    "        print(\"Invalid URL provided. Please include a domain.\")\n",
    "        sys.exit()\n",
    "\n",
    "    print(f\"Starting analysis for website: {start_url}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Find all links on the initial page\n",
    "    found_links = find_all_links(start_url, base_domain)\n",
    "    \n",
    "    # Check for a sitemap\n",
    "    sitemap_urls = check_sitemap(start_url)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Print the links found by crawling\n",
    "    print(f\"\\nLinks found by crawling {start_url} and its internal pages:\")\n",
    "    if found_links:\n",
    "        for link in sorted(list(found_links)):\n",
    "            print(f\"- {link}\")\n",
    "    else:\n",
    "        print(\"No internal links were found.\")\n",
    "        \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Print the links found in the sitemap\n",
    "    print(\"\\nURLs found in the sitemap:\")\n",
    "    if sitemap_urls:\n",
    "        for url in sitemap_urls:\n",
    "            print(f\"- {url}\")\n",
    "    else:\n",
    "        print(\"No sitemap was found or processed.\")\n",
    "        \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Compare the two sets of links within the sitemap\n",
    "    combined_links = found_links.union(sitemap_urls)\n",
    "    \n",
    "    if combined_links:\n",
    "        print(\"\\nChecking the status of all unique URLs found...\")\n",
    "        link_statuses = check_link_status(combined_links)\n",
    "        \n",
    "        # Report any bad links (4xx or 5xx) that were found during the crawl or in the sitemap\n",
    "        bad_links = {url: status for url, status in link_statuses.items() if not status.startswith('2') and not status.startswith('3')}\n",
    "        \n",
    "        if bad_links:\n",
    "            print(\"\\n\" + \"=\" * 30)\n",
    "            print(\"BROKEN LINK REPORT\")\n",
    "            print(\"=\" * 30)\n",
    "            for url, status in sorted(bad_links.items()):\n",
    "                print(f\"[{status}] {url}\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\" * 30)\n",
    "            print(\"BROKEN LINK REPORT\")\n",
    "            print(\"=\" * 30)\n",
    "            print(\"No broken links (4xx or 5xx) found!\")\n",
    "        \n",
    "    # Final comparison between crawled and sitemap links\n",
    "    def _parent_url(link: str) -> str:\n",
    "        parsed = urlparse(link)\n",
    "        base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        path = parsed.path.rstrip('/')\n",
    "        if not path:\n",
    "            return f\"{base}/\"\n",
    "        segments = [segment for segment in path.split('/') if segment]\n",
    "        if len(segments) <= 1:\n",
    "            return f\"{base}/\"\n",
    "        parent_path = '/' + '/'.join(segments[:-1])\n",
    "        return f\"{base}{parent_path}\"\n",
    "\n",
    "    def _summarize(source_name: str, urls) -> None:\n",
    "        url_set = set(urls or [])\n",
    "        if not url_set:\n",
    "            print(f\"\\nNo URLs available for {source_name}.\")\n",
    "            return\n",
    "\n",
    "        parent_map: Dict[str, Set[str]] = {}\n",
    "        for link in url_set:\n",
    "            parent = _parent_url(link)\n",
    "            parent_map.setdefault(parent, set()).add(link)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 30)\n",
    "        print(f\"{source_name.upper()} PARENT/SUBLINK COUNTS\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Total parents: {len(parent_map)}\")\n",
    "        print(f\"Total URLs: {len(url_set)}\")\n",
    "        for parent, children in sorted(parent_map.items()):\n",
    "            child_list = sorted(children)\n",
    "            print(f\"- {parent}: {len(child_list)} sublinks\")\n",
    "            for child in child_list:\n",
    "                print(f\"    - {child}\")\n",
    "\n",
    "    _summarize(\"Crawled URLs\", found_links)\n",
    "    _summarize(\"Sitemap URLs\", sitemap_urls)\n",
    "    _summarize(\"All URLs\", combined_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9277b",
   "metadata": {},
   "source": [
    "## JavaScript Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6950cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Starting JavaScript Component Crawl on: https://www.wgtn.ac.nz\n",
      "Restricted to base domain: https://www.wgtn.ac.nz\n",
      "Max pages to visit: 2000000\n",
      "========================================\n",
      "\n",
      "==============================\n",
      "JAVASCRIPT COMPONENTS REPORT\n",
      "==============================\n",
      "\n",
      "Page: https://www.wgtn.ac.nz\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: d6d13a21ec41 (length=227)\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: 3dceac6b400c (length=433)\n",
      "  - inline script\n",
      "    type: application/ld+json\n",
      "    inline hash: 39346a0b334c (length=913)\n",
      "  - external src: https://www.wgtn.ac.nz/__data/assets/git_bridge/0005/2009624/dist/assets/index.js\n",
      "    type: module\n",
      "    defer: True\n",
      "    module: True\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: d6d13a21ec41 (length=227)\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: dd8f10c560da (length=814)\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: e7f391bc0e11 (length=192)\n",
      "  - external src: https://www.wgtn.ac.nz/__data/assets/git_bridge/0007/2075596/dist/assets/index.js?h=0089d46\n",
      "    type: module\n",
      "    module: True\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: 2bb9f9558b14 (length=765)\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: 9e1294c1e7f2 (length=2515)\n",
      "  - external src: https://staticcdn.co.nz/embed/embed.js\n",
      "    type: text/javascript\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: 52027211db25 (length=255)\n",
      "  - external src: https://www.wgtn.ac.nz/__data/assets/text_file/0011/1877996/jquery-3.4.1.min.js\n",
      "    type: text/javascript\n",
      "  - external src: https://polyfill-fastly.io/v3/polyfill.min.js\n",
      "    type: text/javascript\n",
      "  - external src: https://www.wgtn.ac.nz/__data/assets/git_bridge/0005/1778018/dist/toolkit.min.js?h=d28d465\n",
      "    type: text/javascript\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: a180116c5d03 (length=1007)\n",
      "  - inline script\n",
      "    type: text/javascript\n",
      "    inline hash: 10789e7ab16c (length=6859)\n",
      "\n",
      "------------------------------\n",
      "SUMMARY\n",
      "------------------------------\n",
      "\n",
      "Unique external script sources:\n",
      "  - https://polyfill-fastly.io/v3/polyfill.min.js\n",
      "  - https://staticcdn.co.nz/embed/embed.js\n",
      "  - https://www.wgtn.ac.nz/__data/assets/git_bridge/0005/1778018/dist/toolkit.min.js?h=d28d465\n",
      "  - https://www.wgtn.ac.nz/__data/assets/git_bridge/0005/2009624/dist/assets/index.js\n",
      "  - https://www.wgtn.ac.nz/__data/assets/git_bridge/0007/2075596/dist/assets/index.js?h=0089d46\n",
      "  - https://www.wgtn.ac.nz/__data/assets/text_file/0011/1877996/jquery-3.4.1.min.js\n",
      "\n",
      "Total inline scripts detected: 11\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from typing import Any, Dict, List, Set\n",
    "import hashlib\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Helper Functions (Required for crawling/parsing)\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"Checks if a URL has a valid scheme (http or https).\"\"\"\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc]) and result.scheme in ('http', 'https')\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Main Component Functions\n",
    "# NOTE: Adjust the max_pages parameter as needed to control depth of the crawl and how much information is requested\n",
    "def crawl_javascript_components(start_url: str, base_domain: str, max_pages: int = 500000) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Crawls a website to identify and collect metadata about JavaScript components\n",
    "    (external scripts and inline blocks).\n",
    "    \"\"\"\n",
    "    visited: Set[str] = set()\n",
    "    queue = deque([start_url])\n",
    "    js_components: Dict[str, List[Dict[str, Any]]] = {}\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url = queue.popleft()\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "\n",
    "        try:\n",
    "            # Send a GET request to the current URL\n",
    "            response = requests.get(current_url, timeout=500)\n",
    "            response.raise_for_status() # Raises an exception for bad status codes (4xx or 5xx)\n",
    "        except requests.RequestException as exc:\n",
    "            print(f\"Error loading {current_url} for JavaScript scan: {exc}\")\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML content\n",
    "        # Ensure you have 'lxml' installed: pip install lxml\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # 1. New links for crawling\n",
    "        # Extract all links (<a> tags)\n",
    "        for anchor in soup.find_all('a', href=True):\n",
    "            href = anchor.get('href')\n",
    "            full_url = urljoin(current_url, href)\n",
    "            normalized_url = urlparse(full_url)._replace(fragment='', query='').geturl()\n",
    "    \n",
    "        # Check if link is valid, on the base domain, and hasn't been visited/queued\n",
    "        if is_valid_url(normalized_url) and urlparse(normalized_url).netloc == base_domain:\n",
    "            if normalized_url not in visited and normalized_url not in queue:\n",
    "                # Add the child page to the queue for future processing\n",
    "                queue.append(normalized_url)\n",
    "\n",
    "        # 2. Extract JavaScript components from the current page\n",
    "        page_scripts: List[Dict[str, Any]] = []\n",
    "        for script in soup.find_all('script'):\n",
    "            raw_src = script.get('src')\n",
    "            normalized_src = ''\n",
    "\n",
    "            # Handle external scripts (with a 'src' attribute)\n",
    "            if raw_src:\n",
    "                full_src = urljoin(current_url, raw_src.strip())\n",
    "                # Normalize the external script URL\n",
    "                normalized_src = urlparse(full_src)._replace(fragment='').geturl()\n",
    "\n",
    "            # Extract custom data-* attributes\n",
    "            data_attrs: Dict[str, str] = {}\n",
    "            for key, value in script.attrs.items():\n",
    "                if key.startswith('data-'):\n",
    "                    data_attrs[key] = ' '.join(value) if isinstance(value, list) else str(value)\n",
    "\n",
    "            # Handle inline scripts (no 'src' attribute)\n",
    "            inline_hash = ''\n",
    "            inline_length = 0\n",
    "            if not raw_src:\n",
    "                script_content = script.string or script.get_text()\n",
    "                if script_content:\n",
    "                    normalized_text = script_content.strip()\n",
    "                    inline_length = len(normalized_text)\n",
    "                    if normalized_text:\n",
    "                        # Hash the content for identification\n",
    "                        inline_hash = hashlib.sha256(normalized_text.encode('utf-8')).hexdigest()[:12]\n",
    "\n",
    "            # Compile script information dictionary\n",
    "            script_info: Dict[str, Any] = {\n",
    "                \"src\": normalized_src,\n",
    "                \"is_external\": bool(raw_src),\n",
    "                \"type\": script.get('type') or 'text/javascript',\n",
    "                \"async\": script.has_attr('async'),\n",
    "                \"defer\": script.has_attr('defer'),\n",
    "                \"module\": script.get('type') == 'module',\n",
    "                \"crossorigin\": script.get('crossorigin') or '',\n",
    "                \"data_attributes\": data_attrs\n",
    "            }\n",
    "            if inline_hash:\n",
    "                script_info[\"inline_hash\"] = inline_hash\n",
    "                script_info[\"inline_length\"] = inline_length\n",
    "\n",
    "            page_scripts.append(script_info)\n",
    "\n",
    "        if page_scripts:\n",
    "            js_components[current_url] = page_scripts\n",
    "\n",
    "    return js_components\n",
    "\n",
    "# Provides a formatted output of the JavaScript components within the site\n",
    "def print_javascript_components(component_map: Dict[str, List[Dict[str, Any]]]) -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted summary of the collected JavaScript components.\n",
    "    \"\"\"\n",
    "    if not component_map:\n",
    "        print(\"No JavaScript components found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"JAVASCRIPT COMPONENTS REPORT\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # 1. Detailed breakdown per page\n",
    "    for page_url, scripts in sorted(component_map.items()):\n",
    "        print(f\"\\nPage: {page_url}\")\n",
    "        for script in scripts:\n",
    "            if script[\"is_external\"]:\n",
    "                print(f\"  - external src: {script['src']}\")\n",
    "            else:\n",
    "                print(\"  - inline script\")\n",
    "            \n",
    "            # Print common attributes\n",
    "            print(f\"    type: {script['type']}\")\n",
    "            if script[\"async\"]:\n",
    "                print(\"    async: True\")\n",
    "            if script[\"defer\"]:\n",
    "                print(\"    defer: True\")\n",
    "            if script[\"module\"]:\n",
    "                print(\"    module: True\")\n",
    "            \n",
    "            # Print conditional attributes\n",
    "            if script[\"crossorigin\"]:\n",
    "                print(f\"    crossorigin: {script['crossorigin']}\")\n",
    "            if script[\"data_attributes\"]:\n",
    "                print(f\"    data-* attrs: {script['data_attributes']}\")\n",
    "            if \"inline_hash\" in script:\n",
    "                print(f\"    inline hash: {script['inline_hash']} (length={script['inline_length']})\")\n",
    "\n",
    "    # 2. Summary statistics\n",
    "    unique_external_set: Set[str] = set()\n",
    "    for scripts in component_map.values():\n",
    "        for script in scripts:\n",
    "            if script['is_external']:\n",
    "                src = script.get('src')\n",
    "                if isinstance(src, str):\n",
    "                    unique_external_set.add(src)\n",
    "\n",
    "    unique_external = sorted(unique_external_set)\n",
    "    inline_total = sum(1 for scripts in component_map.values() for script in scripts if not script['is_external'])\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nUnique external script sources:\")\n",
    "    if unique_external:\n",
    "        for src in unique_external:\n",
    "            print(f\"  - {src}\")\n",
    "    else:\n",
    "        print(\"  None detected.\")\n",
    "\n",
    "    print(f\"\\nTotal inline scripts detected: {inline_total}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # This will take the URL that was provided earlier and use it in this step\n",
    "    TARGET_URL = start_url\n",
    "    TARGET_DOMAIN = start_url # e.g., 'www.google.com'\n",
    "    MAX_PAGES_TO_CRAWL = 2000000 # Set a reasonable limit to prevent excessively long runs\n",
    "\n",
    "    # Note: To run this against a real site, you must have 'requests', 'beautifulsoup4', and 'lxml' installed:\n",
    "    # pip install requests beautifulsoup4 lxml\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Starting JavaScript Component Crawl on: {TARGET_URL}\")\n",
    "    print(f\"Restricted to base domain: {TARGET_DOMAIN}\")\n",
    "    print(f\"Max pages to visit: {MAX_PAGES_TO_CRAWL}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Run the crawler\n",
    "    components = crawl_javascript_components(\n",
    "        start_url=TARGET_URL,\n",
    "        base_domain=TARGET_DOMAIN,\n",
    "        max_pages=MAX_PAGES_TO_CRAWL \n",
    "    )\n",
    "    \n",
    "    # Print the report\n",
    "    print_javascript_components(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d3f35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: outputs\n",
      "Exported 102 crawled links to outputs\\crawled_links.csv\n",
      "Exported 3 sitemap links to outputs\\sitemap_links.csv\n",
      "Exported 6 broken links to outputs\\broken_links.csv\n",
      "Exported 17 JavaScript component records to outputs\\javascript_components.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"outputs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# 1. Export Crawled Links\n",
    "if 'found_links' in locals() and found_links:\n",
    "    crawled_file = os.path.join(output_dir, \"crawled_links.csv\")\n",
    "    with open(crawled_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"URL\"])\n",
    "        for link in sorted(found_links):\n",
    "            writer.writerow([link])\n",
    "    print(f\"Exported {len(found_links)} crawled links to {crawled_file}\")\n",
    "else:\n",
    "    print(\"No crawled links data found to export.\")\n",
    "\n",
    "# 2. Export Sitemap Links\n",
    "if 'sitemap_urls' in locals() and sitemap_urls:\n",
    "    sitemap_file = os.path.join(output_dir, \"sitemap_links.csv\")\n",
    "    with open(sitemap_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"URL\"])\n",
    "        for link in sitemap_urls:\n",
    "            writer.writerow([link])\n",
    "    print(f\"Exported {len(sitemap_urls)} sitemap links to {sitemap_file}\")\n",
    "else:\n",
    "    print(\"No sitemap data found to export.\")\n",
    "\n",
    "# 3. Export Broken Links\n",
    "if 'bad_links' in locals() and bad_links:\n",
    "    broken_file = os.path.join(output_dir, \"broken_links.csv\")\n",
    "    with open(broken_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"URL\", \"Status Code/Error\"])\n",
    "        for url, status in sorted(bad_links.items()):\n",
    "            writer.writerow([url, status])\n",
    "    print(f\"Exported {len(bad_links)} broken links to {broken_file}\")\n",
    "else:\n",
    "    print(\"No broken links data found to export.\")\n",
    "\n",
    "# 4. Export JavaScript Components\n",
    "if 'components' in locals() and components:\n",
    "    js_file = os.path.join(output_dir, \"javascript_components.csv\")\n",
    "    with open(js_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Define headers\n",
    "        headers = [\n",
    "            \"Page URL\", \"Script Type\", \"Source/Hash\", \"Type Attribute\", \n",
    "            \"Async\", \"Defer\", \"Module\", \"Crossorigin\", \"Data Attributes\"\n",
    "        ]\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        count = 0\n",
    "        for page_url, scripts in sorted(components.items()):\n",
    "            for script in scripts:\n",
    "                # Determine script type and identifier\n",
    "                s_type = \"External\" if script[\"is_external\"] else \"Inline\"\n",
    "                identifier = script[\"src\"] if script[\"is_external\"] else f\"Hash: {script.get('inline_hash', 'N/A')}\"\n",
    "                \n",
    "                # Format data attributes as a string\n",
    "                data_attrs_str = \"; \".join([f\"{k}={v}\" for k, v in script[\"data_attributes\"].items()])\n",
    "                \n",
    "                row = [\n",
    "                    page_url,\n",
    "                    s_type,\n",
    "                    identifier,\n",
    "                    script[\"type\"],\n",
    "                    script[\"async\"],\n",
    "                    script[\"defer\"],\n",
    "                    script[\"module\"],\n",
    "                    script[\"crossorigin\"],\n",
    "                    data_attrs_str\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "                count += 1\n",
    "                \n",
    "    print(f\"Exported {count} JavaScript component records to {js_file}\")\n",
    "else:\n",
    "    print(\"No JavaScript component data found to export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
