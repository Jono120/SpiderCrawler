{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7258b5f2",
   "metadata": {},
   "source": [
    "# Site Crawler\n",
    "A Python script to analyze a website's links and sitemap, now with link status checking.\n",
    "- To run this script, you may need to install the required libraries.\n",
    "    - If there is packages needed just uncomment the ones you need and run with the script build\n",
    "\n",
    "The below notebook was built with Python 3.12 as a base runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd940ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following command(s) to install any required libraries:\n",
    "#%pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b90c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Set, List, Dict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb61011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set to store visited URLs to prevent infinite loops on circular links\n",
    "visited_urls: Set[str] = set()\n",
    "\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a URL is valid by parsing it and ensuring it has a scheme and netloc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ce826",
   "metadata": {},
   "source": [
    "## Link Finder\n",
    "This will scan the base domain and check for the level of internal links within the site.\n",
    "It runs a recursive check on the webpage as well as its subpages (children) to provide clear detail to the domain structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f64592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_links(url: str, base_domain: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Recursively finds all internal links on a given webpage and its subpages.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the page to crawl.\n",
    "        base_domain (str): The domain of the website to stay within.\n",
    "    \n",
    "    Returns:\n",
    "        Set[str]: A set of unique internal URLs found.\n",
    "    \"\"\"\n",
    "    if url in visited_urls:\n",
    "        return set()\n",
    "    \n",
    "    print(f\"Crawling: {url}\")\n",
    "    # Add a limit to the number of URLs to crawl to prevent the script from running forever\n",
    "    # adjust the limit for the number of URLs to check as needed\n",
    "    if len(visited_urls) > 1000:\n",
    "        print(\"Crawl limit reached (1000 URLs). Stopping further crawling.\")\n",
    "        return set()\n",
    "    \n",
    "    visited_urls.add(url)\n",
    "    \n",
    "    internal_links: Set[str] = set()\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            full_url = urljoin(url, href)\n",
    "            \n",
    "            # Normalize URL to remove fragments and query parameters\n",
    "            normalized_url = urlparse(full_url)._replace(fragment='', query='').geturl()\n",
    "            \n",
    "            if is_valid_url(normalized_url) and urlparse(normalized_url).netloc == base_domain:\n",
    "                internal_links.add(normalized_url)\n",
    "                \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "        \n",
    "    return internal_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc353b93",
   "metadata": {},
   "source": [
    "## Sitemap check\n",
    "The below section will scan the domain for a sitemap file that can be used as a starting point to what is contained within the site.\n",
    "It will produce the number of what is contained in the sitemap and provide it to the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sitemap(url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Attempts to find and parse a sitemap.xml file for a given URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The base URL of the website.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of URLs found within the sitemap, or an empty list if not found.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    sitemap_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/sitemap.xml\"\n",
    "    \n",
    "    print(f\"\\nAttempting to find sitemap at: {sitemap_url}\")\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        urls_from_sitemap = [loc.text for loc in soup.find_all('loc')]\n",
    "        \n",
    "        if urls_from_sitemap:\n",
    "            print(f\"Found {len(urls_from_sitemap)} URLs in the sitemap.\")\n",
    "            return urls_from_sitemap\n",
    "        else:\n",
    "            print(\"Sitemap found but no URLs were listed.\")\n",
    "            return []\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Sitemap not found or an error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d136d7",
   "metadata": {},
   "source": [
    "## Progress Bar\n",
    "Just a simple progress bar that will show how many pages scanned and how much to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b219b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link_status(urls: Set[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Checks the HTTP status for a set of URLs.\n",
    "    \n",
    "    Args:\n",
    "        urls (Set[str]): A set of URLs to check.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary of URLs and their status codes or error messages.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"CHECKING LINK STATUSES...\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    status_results = {}\n",
    "    total_links = len(urls)\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        # Progress indicator\n",
    "        sys.stdout.write(f\"\\rChecking link {i}/{total_links}: {url[:60]}...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        try:\n",
    "            response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "            status_results[url] = str(response.status_code)\n",
    "        except requests.RequestException as e:\n",
    "            status_results[url] = f\"Error: {e}\"\n",
    "            \n",
    "    sys.stdout.write(\"\\n\") # Newline after progress bar\n",
    "    return status_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35841a18",
   "metadata": {},
   "source": [
    "## URL Request and Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests the user to input a that is wanted to be checked\n",
    "if __name__ == \"__main__\":\n",
    "    # This will ask the user for the URL to analyse\n",
    "    start_url = input(\"Please enter the website URL to check (e.g., https://example.com): \")\n",
    "    if not start_url.startswith('http'):\n",
    "        start_url = 'https://' + start_url\n",
    "    \n",
    "    base_domain = urlparse(start_url).netloc\n",
    "    if not base_domain:\n",
    "        print(\"Invalid URL provided. Please include a domain.\")\n",
    "        sys.exit()\n",
    "\n",
    "    print(f\"Starting analysis for website: {start_url}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Find all links on the initial page\n",
    "    found_links = find_all_links(start_url, base_domain)\n",
    "    \n",
    "    # Check for a sitemap\n",
    "    sitemap_urls = check_sitemap(start_url)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Print the links found by crawling\n",
    "    print(f\"\\nLinks found by crawling {start_url} and its internal pages:\")\n",
    "    if found_links:\n",
    "        for link in sorted(list(found_links)):\n",
    "            print(f\"- {link}\")\n",
    "    else:\n",
    "        print(\"No internal links were found.\")\n",
    "        \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Print the links found in the sitemap\n",
    "    print(\"\\nURLs found in the sitemap:\")\n",
    "    if sitemap_urls:\n",
    "        for url in sitemap_urls:\n",
    "            print(f\"- {url}\")\n",
    "    else:\n",
    "        print(\"No sitemap was found or processed.\")\n",
    "        \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Compare the two sets of links within the sitemap\n",
    "    combined_links = found_links.union(sitemap_urls)\n",
    "    \n",
    "    if combined_links:\n",
    "        print(\"\\nChecking the status of all unique URLs found...\")\n",
    "        link_statuses = check_link_status(combined_links)\n",
    "        \n",
    "        # Report any bad links (4xx or 5xx) that were found during the crawl or in the sitemap\n",
    "        bad_links = {url: status for url, status in link_statuses.items() if not status.startswith('2') and not status.startswith('3')}\n",
    "        \n",
    "        if bad_links:\n",
    "            print(\"\\n\" + \"=\" * 30)\n",
    "            print(\"BROKEN LINK REPORT\")\n",
    "            print(\"=\" * 30)\n",
    "            for url, status in sorted(bad_links.items()):\n",
    "                print(f\"[{status}] {url}\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\" * 30)\n",
    "            print(\"BROKEN LINK REPORT\")\n",
    "            print(\"=\" * 30)\n",
    "            print(\"No broken links (4xx or 5xx) found!\")\n",
    "        \n",
    "    # Final comparison between crawled and sitemap links\n",
    "    def _parent_url(link: str) -> str:\n",
    "        parsed = urlparse(link)\n",
    "        base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        path = parsed.path.rstrip('/')\n",
    "        if not path:\n",
    "            return f\"{base}/\"\n",
    "        segments = [segment for segment in path.split('/') if segment]\n",
    "        if len(segments) <= 1:\n",
    "            return f\"{base}/\"\n",
    "        parent_path = '/' + '/'.join(segments[:-1])\n",
    "        return f\"{base}{parent_path}\"\n",
    "\n",
    "    def _summarize(source_name: str, urls) -> None:\n",
    "        url_set = set(urls or [])\n",
    "        if not url_set:\n",
    "            print(f\"\\nNo URLs available for {source_name}.\")\n",
    "            return\n",
    "\n",
    "        parent_map: Dict[str, Set[str]] = {}\n",
    "        for link in url_set:\n",
    "            parent = _parent_url(link)\n",
    "            parent_map.setdefault(parent, set()).add(link)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 30)\n",
    "        print(f\"{source_name.upper()} PARENT/SUBLINK COUNTS\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Total parents: {len(parent_map)}\")\n",
    "        print(f\"Total URLs: {len(url_set)}\")\n",
    "        for parent, children in sorted(parent_map.items()):\n",
    "            child_list = sorted(children)\n",
    "            print(f\"- {parent}: {len(child_list)} sublinks\")\n",
    "            for child in child_list:\n",
    "                print(f\"    - {child}\")\n",
    "\n",
    "    _summarize(\"Crawled URLs\", found_links)\n",
    "    _summarize(\"Sitemap URLs\", sitemap_urls)\n",
    "    _summarize(\"All URLs\", combined_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9277b",
   "metadata": {},
   "source": [
    "## JavaScript Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Any, Dict, List, Set\n",
    "import hashlib\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Helper Functions (Required for crawling/parsing)\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"Checks if a URL has a valid scheme (http or https).\"\"\"\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc]) and result.scheme in ('http', 'https')\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Main Component Functions\n",
    "# NOTE: Adjust the max_pages parameter as needed to control depth of the crawl and how much information is requested\n",
    "def crawl_javascript_components(start_url: str, base_domain: str, max_pages: int = 5000) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Crawls a website to identify and collect metadata about JavaScript components\n",
    "    (external scripts and inline blocks).\n",
    "    \"\"\"\n",
    "    visited: Set[str] = set()\n",
    "    queue = deque([start_url])\n",
    "    js_components: Dict[str, List[Dict[str, Any]]] = {}\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url = queue.popleft()\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "\n",
    "        try:\n",
    "            # Send a GET request to the current URL\n",
    "            response = requests.get(current_url, timeout=5)\n",
    "            response.raise_for_status() # Raises an exception for bad status codes (4xx or 5xx)\n",
    "        except requests.RequestException as exc:\n",
    "            print(f\"Error loading {current_url} for JavaScript scan: {exc}\")\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML content\n",
    "        # Ensure you have 'lxml' installed: pip install lxml\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # 1. New links for crawling\n",
    "        # Extract all links (<a> tags)\n",
    "        for anchor in soup.find_all('a', href=True):\n",
    "            href = anchor.get('href')\n",
    "            full_url = urljoin(current_url, href)\n",
    "            normalized_url = urlparse(full_url)._replace(fragment='', query='').geturl()\n",
    "    \n",
    "        # Check if link is valid, on the base domain, and hasn't been visited/queued\n",
    "        if is_valid_url(normalized_url) and urlparse(normalized_url).netloc == base_domain:\n",
    "            if normalized_url not in visited and normalized_url not in queue:\n",
    "                # Add the child page to the queue for future processing\n",
    "                queue.append(normalized_url)\n",
    "\n",
    "        # 2. Extract JavaScript components from the current page\n",
    "        page_scripts: List[Dict[str, Any]] = []\n",
    "        for script in soup.find_all('script'):\n",
    "            raw_src = script.get('src')\n",
    "            normalized_src = ''\n",
    "\n",
    "            # Handle external scripts (with a 'src' attribute)\n",
    "            if raw_src:\n",
    "                full_src = urljoin(current_url, raw_src.strip())\n",
    "                # Normalize the external script URL\n",
    "                normalized_src = urlparse(full_src)._replace(fragment='').geturl()\n",
    "\n",
    "            # Extract custom data-* attributes\n",
    "            data_attrs: Dict[str, str] = {}\n",
    "            for key, value in script.attrs.items():\n",
    "                if key.startswith('data-'):\n",
    "                    data_attrs[key] = ' '.join(value) if isinstance(value, list) else str(value)\n",
    "\n",
    "            # Handle inline scripts (no 'src' attribute)\n",
    "            inline_hash = ''\n",
    "            inline_length = 0\n",
    "            if not raw_src:\n",
    "                script_content = script.string or script.get_text()\n",
    "                if script_content:\n",
    "                    normalized_text = script_content.strip()\n",
    "                    inline_length = len(normalized_text)\n",
    "                    if normalized_text:\n",
    "                        # Hash the content for identification\n",
    "                        inline_hash = hashlib.sha256(normalized_text.encode('utf-8')).hexdigest()[:12]\n",
    "\n",
    "            # Compile script information dictionary\n",
    "            script_info: Dict[str, Any] = {\n",
    "                \"src\": normalized_src,\n",
    "                \"is_external\": bool(raw_src),\n",
    "                \"type\": script.get('type') or 'text/javascript',\n",
    "                \"async\": script.has_attr('async'),\n",
    "                \"defer\": script.has_attr('defer'),\n",
    "                \"module\": script.get('type') == 'module',\n",
    "                \"crossorigin\": script.get('crossorigin') or '',\n",
    "                \"data_attributes\": data_attrs\n",
    "            }\n",
    "            if inline_hash:\n",
    "                script_info[\"inline_hash\"] = inline_hash\n",
    "                script_info[\"inline_length\"] = inline_length\n",
    "\n",
    "            page_scripts.append(script_info)\n",
    "\n",
    "        if page_scripts:\n",
    "            js_components[current_url] = page_scripts\n",
    "\n",
    "    return js_components\n",
    "\n",
    "# Provides a formatted output of the JavaScript components within the site\n",
    "def print_javascript_components(component_map: Dict[str, List[Dict[str, Any]]]) -> None:\n",
    "    \"\"\"\n",
    "    Prints a formatted summary of the collected JavaScript components.\n",
    "    \"\"\"\n",
    "    if not component_map:\n",
    "        print(\"No JavaScript components found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"JAVASCRIPT COMPONENTS REPORT\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # 1. Detailed breakdown per page\n",
    "    for page_url, scripts in sorted(component_map.items()):\n",
    "        print(f\"\\nPage: {page_url}\")\n",
    "        for script in scripts:\n",
    "            if script[\"is_external\"]:\n",
    "                print(f\"  - external src: {script['src']}\")\n",
    "            else:\n",
    "                print(\"  - inline script\")\n",
    "            \n",
    "            # Print common attributes\n",
    "            print(f\"    type: {script['type']}\")\n",
    "            if script[\"async\"]:\n",
    "                print(\"    async: True\")\n",
    "            if script[\"defer\"]:\n",
    "                print(\"    defer: True\")\n",
    "            if script[\"module\"]:\n",
    "                print(\"    module: True\")\n",
    "            \n",
    "            # Print conditional attributes\n",
    "            if script[\"crossorigin\"]:\n",
    "                print(f\"    crossorigin: {script['crossorigin']}\")\n",
    "            if script[\"data_attributes\"]:\n",
    "                print(f\"    data-* attrs: {script['data_attributes']}\")\n",
    "            if \"inline_hash\" in script:\n",
    "                print(f\"    inline hash: {script['inline_hash']} (length={script['inline_length']})\")\n",
    "\n",
    "    # 2. Summary statistics\n",
    "    unique_external_set: Set[str] = set()\n",
    "    for scripts in component_map.values():\n",
    "        for script in scripts:\n",
    "            if script['is_external']:\n",
    "                src = script.get('src')\n",
    "                if isinstance(src, str):\n",
    "                    unique_external_set.add(src)\n",
    "\n",
    "    unique_external = sorted(unique_external_set)\n",
    "    inline_total = sum(1 for scripts in component_map.values() for script in scripts if not script['is_external'])\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nUnique external script sources:\")\n",
    "    if unique_external:\n",
    "        for src in unique_external:\n",
    "            print(f\"  - {src}\")\n",
    "    else:\n",
    "        print(\"  None detected.\")\n",
    "\n",
    "    print(f\"\\nTotal inline scripts detected: {inline_total}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # This will take the URL that was provided earlier and use it in this step\n",
    "    TARGET_URL = start_url\n",
    "    TARGET_DOMAIN = start_url # e.g., 'www.google.com'\n",
    "    MAX_PAGES_TO_CRAWL = 2000 # Set a reasonable limit to prevent excessively long runs\n",
    "\n",
    "    # Note: To run this against a real site, you must have 'requests', 'beautifulsoup4', and 'lxml' installed:\n",
    "    # pip install requests beautifulsoup4 lxml\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Starting JavaScript Component Crawl on: {TARGET_URL}\")\n",
    "    print(f\"Restricted to base domain: {TARGET_DOMAIN}\")\n",
    "    print(f\"Max pages to visit: {MAX_PAGES_TO_CRAWL}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Run the crawler\n",
    "    components = crawl_javascript_components(\n",
    "        start_url=TARGET_URL,\n",
    "        base_domain=TARGET_DOMAIN,\n",
    "        max_pages=MAX_PAGES_TO_CRAWL \n",
    "    )\n",
    "    \n",
    "    # Print the report\n",
    "    print_javascript_components(components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
